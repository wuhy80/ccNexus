# Token ç»Ÿè®¡ä¿®å¤å¿…è¦æ€§éªŒè¯

## é—®é¢˜ç¡®è®¤

ç”¨æˆ·æŠ¥å‘Šï¼š"æœ€è¿‘è¯·æ±‚"å³è¾¹ä¸‹é¢ä¸€è¡Œå§‹ç»ˆæ˜¾ç¤º `0/0`ï¼Œå³ä½¿è¯·æ±‚æˆåŠŸã€‚

## æ•°æ®æµè¿½è¸ª

### 1. è¯·æ±‚é˜¶æ®µ âœ…

**æ–‡ä»¶**: `internal/transformer/convert/claude_openai.go:142-144`

```go
// Enable usage tracking for streaming
if req.Stream {
    openaiReq.StreamOptions = &transformer.StreamOptions{IncludeUsage: true}
}
```

**ç»“è®º**: ä»£ç å·²æ­£ç¡®è®¾ç½® `stream_options.include_usage = true`ï¼ŒOpenAI ä¼šè¿”å› usage ä¿¡æ¯ã€‚

**å‚è€ƒ**: [OpenAI å®˜æ–¹å…¬å‘Š](https://community.openai.com/t/usage-stats-now-available-when-using-streaming-with-the-chat-completions-api-or-completions-api/738156) - "Usage stats now available when using streaming"

---

### 2. OpenAI å“åº”é˜¶æ®µ âœ…

**OpenAI æµå¼å“åº”ç»“æ„** (`internal/transformer/types.go:75-96`):

```go
type OpenAIStreamChunk struct {
    ID      string `json:"id"`
    Object  string `json:"object"`
    Created int64  `json:"created"`
    Model   string `json:"model"`
    Choices []struct {
        Index int `json:"index"`
        Delta struct {
            Role    string `json:"role,omitempty"`
            Content string `json:"content,omitempty"`
            // ...
        } `json:"delta"`
        FinishReason *string `json:"finish_reason"`
    } `json:"choices"`
    Usage *struct {
        PromptTokens     int `json:"prompt_tokens"`      // â† è¾“å…¥ Token
        CompletionTokens int `json:"completion_tokens"`  // â† è¾“å‡º Token
        TotalTokens      int `json:"total_tokens"`
    } `json:"usage,omitempty"`  // â† åœ¨æœ€åä¸€ä¸ª chunk ä¸­è¿”å›
}
```

**ç»“è®º**: OpenAI åœ¨å¸¦ `finish_reason` çš„æœ€åä¸€ä¸ª chunk ä¸­è¿”å› `usage` å¯¹è±¡ã€‚

---

### 3. è½¬æ¢å™¨é˜¶æ®µ âŒ **é—®é¢˜æ‰€åœ¨**

**æ–‡ä»¶**: `internal/transformer/convert/claude_openai.go:428-547`

#### é—®é¢˜ä»£ç  1: message_start äº‹ä»¶ï¼ˆç¬¬ 467-474 è¡Œï¼‰

```go
result = append(result, buildClaudeEvent("message_start", map[string]interface{}{
    "message": map[string]interface{}{
        "id": chunk.ID, "type": "message", "role": "assistant",
        "content": []interface{}{}, "model": ctx.ModelName,
        "stop_reason": nil, "stop_sequence": nil,
        "usage": map[string]interface{}{
            "input_tokens": 0,   // â† ç¡¬ç¼–ç ä¸º 0ï¼
            "output_tokens": 0   // â† ç¡¬ç¼–ç ä¸º 0ï¼
        },
    },
})...)
```

**é—®é¢˜**: å¿½ç•¥äº† `chunk.Usage.PromptTokens`ï¼Œç¡¬ç¼–ç ä¸º 0ã€‚

#### é—®é¢˜ä»£ç  2: message_delta äº‹ä»¶ï¼ˆç¬¬ 539-542 è¡Œï¼‰

```go
result = append(result, buildClaudeEvent("message_delta", map[string]interface{}{
    "delta": map[string]interface{}{"stop_reason": stopReason, "stop_sequence": nil},
    "usage": map[string]interface{}{
        "output_tokens": 0  // â† ç¡¬ç¼–ç ä¸º 0ï¼
    },
})...)
```

**é—®é¢˜**: å¿½ç•¥äº† `chunk.Usage.CompletionTokens`ï¼Œç¡¬ç¼–ç ä¸º 0ã€‚

---

### 4. Token æå–é˜¶æ®µ âŒ **å—å½±å“**

**æ–‡ä»¶**: `internal/proxy/streaming.go:254-287`

```go
func (p *Proxy) extractTokensFromEvent(eventData []byte, usage *transformer.TokenUsageDetail) {
    // ...
    eventType, _ := event["type"].(string)
    if eventType == "message_start" {
        if message, ok := event["message"].(map[string]interface{}); ok {
            if usageMap, ok := message["usage"].(map[string]interface{}); ok {
                detail := transformer.ExtractTokenUsageDetail(usageMap)
                usage.InputTokens = detail.InputTokens  // â† æå–åˆ°çš„æ˜¯ 0
                // ...
            }
        }
    } else if eventType == "message_delta" {
        if usageMap, ok := event["usage"].(map[string]interface{}); ok {
            if output, ok := usageMap["output_tokens"].(float64); ok {
                usage.OutputTokens = int(output)  // â† æå–åˆ°çš„æ˜¯ 0
            }
        }
    }
}
```

**ç»“è®º**: è¿™ä¸ªå‡½æ•°å·¥ä½œæ­£å¸¸ï¼Œä½†å®ƒæå–çš„æ˜¯**è½¬æ¢åçš„ Claude æ ¼å¼äº‹ä»¶**ã€‚å¦‚æœè½¬æ¢å™¨è¾“å‡º 0ï¼Œè¿™é‡Œå°±æå–åˆ° 0ã€‚

---

### 5. æ•°æ®åº“å­˜å‚¨é˜¶æ®µ âŒ **å—å½±å“**

**æ–‡ä»¶**: `internal/proxy/proxy.go:1074-1087`

```go
p.stats.RecordRequestStat(&RequestStatRecord{
    EndpointName:        endpoint.Name,
    ClientType:          string(clientType),
    ClientIP:            clientIP,
    Timestamp:           time.Now(),
    InputTokens:         usage.InputTokens,   // â† å­˜å‚¨çš„æ˜¯ 0
    CacheCreationTokens: usage.CacheCreationInputTokens,
    CacheReadTokens:     usage.CacheReadInputTokens,
    OutputTokens:        usage.OutputTokens,  // â† å­˜å‚¨çš„æ˜¯ 0
    Model:               streamReq.Model,
    IsStreaming:         true,
    Success:             true,
    DurationMs:          durationMs,
})
```

**ç»“è®º**: æ•°æ®åº“ä¸­å­˜å‚¨çš„å°±æ˜¯ 0ã€‚

---

### 6. å‰ç«¯æ˜¾ç¤ºé˜¶æ®µ âŒ **å—å½±å“**

**æ–‡ä»¶**: `cmd/desktop/frontend/src/modules/monitor.js:343-344`

```javascript
inputTokens: req.inputTokens + req.cacheCreationTokens + req.cacheReadTokens,
outputTokens: req.outputTokens,
```

**ç»“è®º**: å‰ç«¯æ˜¾ç¤ºçš„æ˜¯æ•°æ®åº“ä¸­çš„å€¼ï¼Œå³ 0/0ã€‚

---

## ä¿®å¤éªŒè¯

### ä¿®å¤å‰çš„æ•°æ®æµ

```
OpenAI API è¿”å›:
  chunk.Usage.PromptTokens = 1234
  chunk.Usage.CompletionTokens = 567

â†“ è½¬æ¢å™¨ (claude_openai.go)
  ç¡¬ç¼–ç : input_tokens = 0, output_tokens = 0  â† é—®é¢˜ï¼

â†“ extractTokensFromEvent
  æå–: InputTokens = 0, OutputTokens = 0

â†“ æ•°æ®åº“
  å­˜å‚¨: input_tokens = 0, output_tokens = 0

â†“ å‰ç«¯
  æ˜¾ç¤º: 0 / 0
```

### ä¿®å¤åçš„æ•°æ®æµ

```
OpenAI API è¿”å›:
  chunk.Usage.PromptTokens = 1234
  chunk.Usage.CompletionTokens = 567

â†“ è½¬æ¢å™¨ (claude_openai.go) - å·²ä¿®å¤
  ä» chunk.Usage æå–: input_tokens = 1234, output_tokens = 567  â† ä¿®å¤ï¼

â†“ extractTokensFromEvent
  æå–: InputTokens = 1234, OutputTokens = 567

â†“ æ•°æ®åº“
  å­˜å‚¨: input_tokens = 1234, output_tokens = 567

â†“ å‰ç«¯
  æ˜¾ç¤º: 1234 / 567  â† æ­£ç¡®ï¼
```

---

## å…¶ä»–è½¬æ¢å™¨éªŒè¯

### OpenAI2 è½¬æ¢å™¨

**æ–‡ä»¶**: `internal/transformer/convert/claude_openai2.go`

**é—®é¢˜**: åŒæ ·ç¡¬ç¼–ç ä¸º 0ï¼ˆç¬¬ 441 å’Œ 501 è¡Œï¼‰

**OpenAI2 å“åº”ç»“æ„** (`types.go:431-441`):
```go
type OpenAI2Response struct {
    // ...
    Usage struct {
        InputTokens  int `json:"input_tokens"`   // â† æœ‰æ•°æ®
        OutputTokens int `json:"output_tokens"`  // â† æœ‰æ•°æ®
        TotalTokens  int `json:"total_tokens"`
    } `json:"usage"`
}
```

**ç»“è®º**: éœ€è¦ä¿®å¤ã€‚

---

### Gemini è½¬æ¢å™¨

**æ–‡ä»¶**: `internal/transformer/convert/claude_gemini.go`

**é—®é¢˜**: åŒæ ·ç¡¬ç¼–ç ä¸º 0ï¼ˆç¬¬ 381 å’Œ 440 è¡Œï¼‰

**Gemini å“åº”ç»“æ„** (`types.go:353-357`):
```go
UsageMetadata *struct {
    PromptTokenCount     int `json:"promptTokenCount"`      // â† æœ‰æ•°æ®
    CandidatesTokenCount int `json:"candidatesTokenCount"`  // â† æœ‰æ•°æ®
    TotalTokenCount      int `json:"totalTokenCount"`
} `json:"usageMetadata,omitempty"`
```

**ç»“è®º**: éœ€è¦ä¿®å¤ã€‚

---

## æœ€ç»ˆç»“è®º

### âœ… ä¿®æ”¹æ˜¯å¿…è¦çš„

**åŸå› **:

1. **OpenAI/OpenAI2/Gemini API éƒ½è¿”å› usage ä¿¡æ¯**
   - OpenAI: é€šè¿‡ `stream_options.include_usage = true` å¯ç”¨
   - OpenAI2: åœ¨ `response.completed` äº‹ä»¶ä¸­
   - Gemini: åœ¨ `usageMetadata` å­—æ®µä¸­

2. **è½¬æ¢å™¨ç¡¬ç¼–ç ä¸º 0ï¼Œä¸¢å¼ƒäº†çœŸå®æ•°æ®**
   - è¿™æ˜¯ Bugï¼Œä¸æ˜¯è®¾è®¡æ„å›¾
   - å¯¼è‡´æ•´ä¸ªç»Ÿè®¡é“¾è·¯å¤±æ•ˆ

3. **ç”¨æˆ·çœ‹åˆ°çš„ 0/0 æ˜¯çœŸå®é—®é¢˜**
   - ä¸æ˜¯å‰ç«¯é—®é¢˜
   - ä¸æ˜¯æ•°æ®åº“é—®é¢˜
   - æ˜¯è½¬æ¢å™¨é—®é¢˜

4. **ä¿®å¤ç®€å•ä¸”å®‰å…¨**
   - åªéœ€ä»åŸå§‹å“åº”ä¸­æå–çœŸå®å€¼
   - ä¿ç•™åå¤‡æœºåˆ¶ï¼ˆä½¿ç”¨é¢„ä¼°å€¼ï¼‰
   - ä¸å½±å“å…¶ä»–åŠŸèƒ½

### ğŸ“Š å½±å“èŒƒå›´

- **å—å½±å“çš„ç«¯ç‚¹ç±»å‹**: ä½¿ç”¨ `cc_openai`, `cc_openai2`, `cc_gemini` è½¬æ¢å™¨çš„ç«¯ç‚¹
- **ä¸å—å½±å“**: `cc_claude` è½¬æ¢å™¨ï¼ˆç›´æ¥ä½¿ç”¨ Claude APIï¼‰
- **ä¸å—å½±å“**: Codex è½¬æ¢å™¨ï¼ˆä¸åŒçš„å®ç°ï¼‰

### ğŸ¯ ä¿®å¤æ•ˆæœ

ä¿®å¤åï¼Œç”¨æˆ·å°†çœ‹åˆ°ï¼š
- âœ… æ­£ç¡®çš„è¾“å…¥ Token æ•°é‡
- âœ… æ­£ç¡®çš„è¾“å‡º Token æ•°é‡
- âœ… å‡†ç¡®çš„æˆæœ¬ä¼°ç®—
- âœ… æœ‰æ„ä¹‰çš„ä½¿ç”¨ç»Ÿè®¡

---

## å‚è€ƒèµ„æ–™

1. [OpenAI - Usage stats now available when using streaming](https://community.openai.com/t/usage-stats-now-available-when-using-streaming-with-the-chat-completions-api-or-completions-api/738156)
2. [Stack Overflow - How to get token usage in streaming mode](https://stackoverflow.com/questions/75824798/how-to-get-token-usage-for-each-openai-chatcompletion-api-call-in-streaming-mode)
3. [Medium - Calculate OpenAI usage for Chat Completion API stream](https://medium.com/@votanlean/calculate-openai-usage-for-chat-completion-api-stream-in-nodejs-03eb9172d407)
